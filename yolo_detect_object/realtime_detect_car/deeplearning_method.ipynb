{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "866b18d4",
   "metadata": {},
   "source": [
    "# PyTorch method for car detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04f91bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import yaml\n",
    "from skimage.io import imread\n",
    "import imagesize\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import SGD, AdamW\n",
    "from torch.optim.lr_scheduler import MultiStepLR, CosineAnnealingLR, OneCycleLR\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torchvision\n",
    "import torchvision.models.detection as detection\n",
    "from torchvision import transforms\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbec6aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloDataset(Dataset):\n",
    "    def __init__(self, img_dir, label_dir, transform=None, mosaic_prob=0.5, mixup_prob=0.2):\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.img_files = [f for f in os.listdir(img_dir) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "        self.transform = transform\n",
    "        self.mosaic_prob = mosaic_prob\n",
    "        self.mixup_prob = mixup_prob\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "    \n",
    "    def load_image_and_labels(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_files[idx])\n",
    "        label_path = os.path.join(self.label_dir, self.img_files[idx].replace('.jpg', '.txt').replace('.png', '.txt'))\n",
    "        \n",
    "        # Load image\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        h, w = img.shape[:2]\n",
    "        \n",
    "        # Load labels\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f.readlines():\n",
    "                    if line.strip():\n",
    "                        cls, x_c, y_c, width, height = map(float, line.strip().split())\n",
    "                        # Convert to absolute coordinates\n",
    "                        x1 = (x_c - width/2) * w\n",
    "                        y1 = (y_c - height/2) * h\n",
    "                        x2 = (x_c + width/2) * w\n",
    "                        y2 = (y_c + height/2) * h\n",
    "                        boxes.append([x1, y1, x2, y2])\n",
    "                        labels.append(int(cls) + 1)\n",
    "        \n",
    "        return img, boxes, labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img, boxes, labels = self.load_image_and_labels(idx)\n",
    "        # Apply augmentations\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=img, bboxes=boxes, labels=labels)\n",
    "            img = transformed['image']\n",
    "            boxes = transformed['bboxes']\n",
    "            labels = transformed['labels']\n",
    "        \n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32) if boxes else torch.empty((0, 4), dtype=torch.float32)\n",
    "        labels = torch.tensor(labels, dtype=torch.int64) if labels else torch.empty((0,), dtype=torch.int64)\n",
    "        \n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"image_id\": torch.tensor([idx])\n",
    "        }\n",
    "        \n",
    "        return img, target\n",
    "    \n",
    "    def get_augmentation_transforms(training=True):\n",
    "        if training:\n",
    "            return A.Compose([\n",
    "                A.RandomResizedCrop(height=800, width=800, scale=(0.8, 1.0), ratio=(0.75, 1.33)),\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.VerticalFlip(p=0.1),\n",
    "                A.Rotate(limit=15, p=0.3),\n",
    "                A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "                A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.3),\n",
    "                A.GaussNoise(var_limit=(10.0, 50.0), p=0.2),\n",
    "                A.GaussianBlur(blur_limit=3, p=0.1),\n",
    "                A.Cutout(num_holes=8, max_h_size=32, max_w_size=32, fill_value=0, p=0.2),\n",
    "                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                ToTensorV2()\n",
    "            ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels'], min_visibility=0.3))\n",
    "        else:\n",
    "            return A.Compose([\n",
    "                A.Resize(height=800, width=800),\n",
    "                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                ToTensorV2()\n",
    "            ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n",
    "\n",
    "    def build_optimized_model(num_classes, backbone='resnet50', pretrained=True):\n",
    "        \"\"\"Build model with different backbone options\"\"\"\n",
    "        if backbone == 'resnet50':\n",
    "            model = detection.fasterrcnn_resnet50_fpn(pretrained=pretrained)\n",
    "        elif backbone == 'resnet101':\n",
    "            model = detection.fasterrcnn_resnet101_fpn(pretrained=pretrained)\n",
    "        elif backbone == 'mobilenet':\n",
    "            model = detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=pretrained)\n",
    "        \n",
    "        in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "        model.roi_heads.box_predictor = detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "        \n",
    "        return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
