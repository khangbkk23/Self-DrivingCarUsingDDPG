{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "866b18d4",
   "metadata": {},
   "source": [
    "# PyTorch method for car detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04f91bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import yaml\n",
    "from skimage.io import imread\n",
    "import imagesize\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import SGD, AdamW\n",
    "from torch.optim.lr_scheduler import MultiStepLR, CosineAnnealingLR, OneCycleLR\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torchvision\n",
    "import torchvision.models.detection as detection\n",
    "from torchvision import transforms\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbec6aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloDataset(Dataset):\n",
    "    def __init__(self, img_dir, label_dir, transform=None, mosaic_prob=0.5, mixup_prob=0.2):\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.img_files = [f for f in os.listdir(img_dir) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "        self.transform = transform\n",
    "        self.mosaic_prob = mosaic_prob\n",
    "        self.mixup_prob = mixup_prob\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "    \n",
    "    def load_image_and_labels(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_files[idx])\n",
    "        label_path = os.path.join(self.label_dir, self.img_files[idx].replace('.jpg', '.txt').replace('.png', '.txt'))\n",
    "        \n",
    "        # Load image\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        h, w = img.shape[:2]\n",
    "        \n",
    "        # Load labels\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f.readlines():\n",
    "                    if line.strip():\n",
    "                        cls, x_c, y_c, width, height = map(float, line.strip().split())\n",
    "                        # Convert to absolute coordinates\n",
    "                        x1 = (x_c - width/2) * w\n",
    "                        y1 = (y_c - height/2) * h\n",
    "                        x2 = (x_c + width/2) * w\n",
    "                        y2 = (y_c + height/2) * h\n",
    "                        boxes.append([x1, y1, x2, y2])\n",
    "                        labels.append(int(cls) + 1)\n",
    "        \n",
    "        return img, boxes, labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img, boxes, labels = self.load_image_and_labels(idx)\n",
    "        # Apply augmentations\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=img, bboxes=boxes, labels=labels)\n",
    "            img = transformed['image']\n",
    "            boxes = transformed['bboxes']\n",
    "            labels = transformed['labels']\n",
    "        \n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32) if boxes else torch.empty((0, 4), dtype=torch.float32)\n",
    "        labels = torch.tensor(labels, dtype=torch.int64) if labels else torch.empty((0,), dtype=torch.int64)\n",
    "        \n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"image_id\": torch.tensor([idx])\n",
    "        }\n",
    "        \n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7ce74af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_augmentation_transforms(training=True):\n",
    "        if training:\n",
    "            return A.Compose([\n",
    "                A.RandomResizedCrop(height=800, width=800, scale=(0.8, 1.0), ratio=(0.75, 1.33)),\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.VerticalFlip(p=0.1),\n",
    "                A.Rotate(limit=15, p=0.3),\n",
    "                A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "                A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.3),\n",
    "                A.GaussNoise(var_limit=(10.0, 50.0), p=0.2),\n",
    "                A.GaussianBlur(blur_limit=3, p=0.1),\n",
    "                A.Cutout(num_holes=8, max_h_size=32, max_w_size=32, fill_value=0, p=0.2),\n",
    "                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                ToTensorV2()\n",
    "            ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels'], min_visibility=0.3))\n",
    "        else:\n",
    "            return A.Compose([\n",
    "                A.Resize(height=800, width=800),\n",
    "                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                ToTensorV2()\n",
    "            ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n",
    "\n",
    "def build_optimized_model(num_classes, backbone='resnet50', pretrained=True):\n",
    "    if backbone == 'resnet50':\n",
    "        model = detection.fasterrcnn_resnet50_fpn(pretrained=pretrained)\n",
    "    elif backbone == 'resnet101':\n",
    "        model = detection.fasterrcnn_resnet101_fpn(pretrained=pretrained)\n",
    "    elif backbone == 'mobilenet':\n",
    "        model = detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=pretrained)\n",
    "        \n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39185dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, min_delta=0, restore_best_weights=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.best_weights = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(model)\n",
    "        elif self.best_loss - val_loss > self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.save_checkpoint(model)\n",
    "        else:\n",
    "            self.counter += 1\n",
    "\n",
    "        if self.counter >= self.patience:\n",
    "            if self.restore_best_weights:\n",
    "                model.load_state_dict(self.best_weights)\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def save_checkpoint(self, model):\n",
    "        self.best_weights = model.state_dict().copy()\n",
    "        \n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0212cb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "def train_one_epoch(model, optimizer, dataloader, device, scaler, epoch, scheduler=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    loss_components = defaultdict(float)\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=f'Epoch {epoch}')\n",
    "    \n",
    "    for batch_idx, (imgs, targets) in enumerate(progress_bar):\n",
    "        imgs = list(img.to(device) for img in imgs)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast():\n",
    "            loss_dict = model(imgs, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        scaler.scale(losses).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        total_loss += losses.item()\n",
    "        \n",
    "        # Log individual loss components\n",
    "        for k, v in loss_dict.items():\n",
    "            loss_components[k] += v.item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{losses.item():.4f}',\n",
    "            'avg_loss': f'{total_loss/(batch_idx+1):.4f}'\n",
    "        })\n",
    "        \n",
    "        # Step scheduler if it's OneCycleLR\n",
    "        if scheduler and isinstance(scheduler, OneCycleLR):\n",
    "            scheduler.step()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_loss_components = {k: v / len(dataloader) for k, v in loss_components.items()}\n",
    "    \n",
    "    return avg_loss, avg_loss_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8617c30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for imgs, targets in tqdm(dataloader, desc='Validation'):\n",
    "            imgs = list(img.to(device) for img in imgs)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            with autocast():\n",
    "                loss_dict = model(imgs, targets)\n",
    "                losses = sum(loss for loss in loss_dict.values())\n",
    "            \n",
    "            total_loss += losses.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc407cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, optimizer_type='sgd', lr=0.005, weight_decay=0.0005):\n",
    "    if optimizer_type.lower() == 'sgd':\n",
    "        return SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "    elif optimizer_type.lower() == 'adamw':\n",
    "        return AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported optimizer: {optimizer_type}\")\n",
    "\n",
    "def get_scheduler(optimizer, scheduler_type='multistep', epochs=100, steps_per_epoch=None):\n",
    "    if scheduler_type.lower() == 'multistep':\n",
    "        milestones = [int(epochs * 0.7), int(epochs * 0.9)]\n",
    "        return MultiStepLR(optimizer, milestones=milestones, gamma=0.1)\n",
    "    elif scheduler_type.lower() == 'cosine':\n",
    "        return CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    elif scheduler_type.lower() == 'onecycle':\n",
    "        return OneCycleLR(optimizer, max_lr=0.01, epochs=epochs, steps_per_epoch=steps_per_epoch)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8f2f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    config = {\n",
    "        'batch_size': 8,\n",
    "        'num_epochs': 100,\n",
    "        'num_classes': 2,\n",
    "        'learning_rate': 0.01,\n",
    "        'optimizer': 'adamw',\n",
    "        'scheduler': 'onecycle',\n",
    "        'backbone': 'resnet50',\n",
    "        'patience': 15,\n",
    "        'img_size': 800,\n",
    "        'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    }\n",
    "    \n",
    "    device = torch.device(config['device'])\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Data augmentation\n",
    "    train_transforms = get_augmentation_transforms(training=True)\n",
    "    val_transforms = get_augmentation_transforms(training=False)\n",
    "    \n",
    "    # Datasets\n",
    "    train_dataset = YoloDataset(\"data/train/images\", \"data/train/labels\", transform=train_transforms)\n",
    "    val_dataset = YoloDataset(\"data/val/images\", \"data/val/labels\", transform=val_transforms)\n",
    "    \n",
    "    # Data loaders with optimized settings\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=config['batch_size'], \n",
    "        shuffle=True, \n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=4,  # Parallel data loading\n",
    "        pin_memory=True,  # Faster GPU transfer\n",
    "        persistent_workers=True  # Keep workers alive\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=config['batch_size'], \n",
    "        shuffle=False, \n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Model\n",
    "    model = build_optimized_model(config['num_classes'], backbone=config['backbone'])\n",
    "    model.to(device)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = get_optimizer(model, config['optimizer'], config['learning_rate'])\n",
    "    \n",
    "    # Scheduler\n",
    "    scheduler = get_scheduler(\n",
    "        optimizer, \n",
    "        config['scheduler'], \n",
    "        config['num_epochs'], \n",
    "        len(train_loader)\n",
    "    )\n",
    "    \n",
    "    # Mixed precision scaler\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stopping = EarlyStopping(patience=config['patience'])\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(config['num_epochs']):\n",
    "        # Training\n",
    "        train_loss, loss_components = train_one_epoch(\n",
    "            model, optimizer, train_loader, device, scaler, epoch, scheduler\n",
    "        )\n",
    "        \n",
    "        # Validation\n",
    "        val_loss = validate_model(model, val_loader, device)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Step scheduler (except OneCycleLR which steps during training)\n",
    "        if scheduler and not isinstance(scheduler, OneCycleLR):\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Logging\n",
    "        print(f\"Epoch {epoch+1}/{config['num_epochs']}:\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "        for component, value in loss_components.items():\n",
    "            print(f\"  {component}: {value:.4f}\")\n",
    "        print(f\"  LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "                'config': config\n",
    "            }, 'best_model.pth')\n",
    "            print(f\"New best model saved with val_loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if early_stopping(val_loss, model):\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "    \n",
    "    print(\"Training completed!\")\n",
    "    return model, train_losses, val_losses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
